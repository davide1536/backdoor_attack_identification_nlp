# Introduction
This project is aimed to tackle the problem of backdoor attacks in the deep learning field.
Backdoor attack has the goal to inject a backdoor inside a neural network in order to avoid the security provided by the system. The backdoor is injected from an adversary by poisoning a small part of the dataset where the neural network is trained. Once the dataset is poisoned the adversary can easily use the vulnerability by giving to the system a poison sample that contains specific triggers that allow to activate the backdoor and so produce unexpected outputs.
In this work we will propose a method in order to try to detect if a sample is poisoned or not. We have worked in a specific case scenario (toxic comment classification) in which the adversary uses PPLM (Plug and Play language Model).
## Project Description
Our work has the aim to detect most of the bad samples produced by the plug and play language model, but can be extended also to a more general case at the cost of having an higher rate of false positive.

We proposed 2 different binary classification approaches - SVM with RBF Kernel and a simple binary neural network. Both of them are trained on a labeled dataset that consist of clear and poisoned data (generated by PPLM), and our goal is to build a binary classifier that is able to separate poisoned samples from good samples. We achieved good performances with both classifiers. In particular, if we test our model on a dataset that contains the same type of bad samples, i.e. generated from the same network, from which the model has been trained on, we can identify, in average, $\approx 85\%$ of backdoor samples. On the other hand the things become harder on identifying samples that are generated by another architecture e.g. LSTM model. In this case we were able to detect  $\approx 50\%$ of bad samples, using a model that was trained on a dataset where bad samples were generated using Plug and Play Language Model. 

## Goals
The main goals in this project are:
- Build a meaningful representation for sentences, such that poisoned sampels can be detected
- Build classifiers that exploit the built representation in order to understand if a sample is poisoned or not

# Implementation
To identify malicious samples we had to follow different steps:
 - Data collection: dataset composed of both benign and poisoned data. For benigh data we selected ["Kaggle Toxic Comment"](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) dataset, a dataset that is publicly available on kaggle. Poisoned data were injected using the code provided ["Li et al."](https://github.com/lishaofeng/NLP_Backdoor). (npl_backdoor.py)
 - Data representation: Once collected benign and poisoned data we needed to represent these data in a vector space in order to use them as input for the binary classifiers. The language model that we have chosen in order to accomplish this task is [BERT](https://huggingface.co/docs/transformers/model_doc/bert), a language model from Google. This operation is done in embedding.py
 -Binary classifiers: The selected models for this task are the SVM with rbf kernel and a binary neural network with 1 hidden layer and 16 hidden units. These models will take in input the built representation and produce a probability indicating if a sample is poisoned or not.

 ![Alt](/results/percentage_distribution.png)
As show by the above image, we had to deal with an extremly unbalanced dataset. 
A suitable approach to our problem is to adopt a class weight based approach. The idea is to assign different weights to samples belonging backdoor and non-backdoor data for each of the technique that we have adopted, the higher the weight for each individual sample the more is important for the classifier to classify it correctly. 
 
# Results
The results will be despalyed with confusion matrixes, in order to visualize the number of false positives and false negatives predicted by the classifiers. Each classifier was trained using toxic PPLM generated sentences, and tested on both PPLM generated sentences and LSTM generated sentences (are they able to generalize with sentences generated with another model?).
Due to unbalanced dataset, we used the following metrics:
- Precision
- Recall
- F1 score

Class 1 = poisoned, class 0 = benign
## SVC Results
### PPLM testing
![Alt](/results/svm_total_plm.png)
Metrics:
 - Precision: 1 on class "0", 0.33 on class "1"
 - Recall: 0.96 on class "0", 0.85 on class "1"
 - F1 score: 0.98 on class "0", 0.47 on class "1" 
### LSTM testing
![Alt](/results/smv_lstm.png) 
Here we tested only using poisoned sampels.
Metrics:
- Precision: 0 on class "0", 1 on class "1"
- Recall: 0 on class "0", 0.36 on class "1"
- F1 score: 0 on class "0", 0.53 on class "1"
## Neural network results
### PPLM testing
![Alt](/results/NN_total_plm.png) 
Metrics:
- Precision: 0.99 on class "0", 0.38 on class "1"
- Recall: 0.97 on class "0", 0.73 on class "1"
- F1 score: 0.98 on class "0", 0.50 on class "1"
### LSTM testing
![Alt](/results/NN_lstm_reducted.png) 
Metrics:
- Precision: 0 on class "0", 1 on class "1"
- Recall: 0 on class "0", 0.49 on class "1"
- F1 score: 0 on class "0", 0.66 on class "1"
# Conclusion
In this project we have seen different method and strategies in order to detect backdoor samples. We have considered the case where the user have a thought about the possible architecture that will be used by the adversary during the attack, but also a possible method that can be used by the user if he/she has no idea about the approach used by the adversary.

Full relation: [Bakcdoor attack](cns_paper/Computer_and_network_security.pdf)